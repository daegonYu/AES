pearson: 0.687 qwk: 0.641

pearson: 0.461 	 qwk: 0.262
pearson: 0.434 	 qwk: 0.21
pearson: 0.421 	 qwk: 0.346


1,2,3
pearson: 0.504 	 qwk: 0.465
pearson: 0.492 	 qwk: 0.478
pearson: 0.587 	 qwk: 0.468
논리성 예측 점수 : 43.02점
참신성 예측 점수 : 9.46점
설득력 예측 점수 : 54.29점



--풍부한 근거--
13 ~ 21 (한번 맞는지 확인해볼 필요가 있다.)

pearson: 0.48 	 qwk: 0.43 13 x
pearson: nan 	 qwk: 0.0 14 13
pearson: 0.497 	 qwk: 0.34 15 14
pearson: 0.543 	 qwk: 0.422 16 15
pearson: 0.507 	 qwk: 0.451 17 16
pearson: 0.536 	 qwk: 0.52 18 17
pearson: 0.585 	 qwk: 0.583 19 18      # best
pearson: 0.53 	 qwk: 0.493 20 19
pearson: 0.565 	 qwk: 0.461 21 20

22

--논리성--

pearson: 0.516 	 qwk: 0.394 
pearson: 0.526 	 qwk: 0.427 
pearson: 0.529 	 qwk: 0.431 24    # best
pearson: 0.459 	 qwk: 0.363 
pearson: 0.383 	 qwk: 0.257
pearson: 0.413 	 qwk: 0.321
pearson: 0.321 	 qwk: 0.221
pearson: 0.428 	 qwk: 0.386

--참신성--

pearson: 0.477 	 qwk: 0.3
pearson: 0.376 	 qwk: 0.343
pearson: 0.436 	 qwk: 0.405
pearson: 0.422 	 qwk: 0.417
pearson: 0.414 	 qwk: 0.39
pearson: 0.424 	 qwk: 0.413 36
pearson: 0.389 	 qwk: 0.385
pearson: 0.445 	 qwk: 0.421 38     # best

--설득력--

pearson: 0.53 	 qwk: 0.355
pearson: 0.485 	 qwk: 0.328 41
pearson: 0.483 	 qwk: 0.337
pearson: 0.462 	 qwk: 0.375
pearson: 0.516 	 qwk: 0.466  44     # best
pearson: 0.485 	 qwk: 0.471
pearson: 0.502 	 qwk: 0.447
pearson: 0.454 	 qwk: 0.417




----------------
참신성 30에폭 추가 학습
49~

--best--

pearson: 0.529 	 qwk: 0.431 # 24 논리성 -> 4
pearson: 0.585 	 qwk: 0.583 # 18 풍부한 근거 -> 실수로 삭제 다시 학습 필요
pearson: 0.516 	 qwk: 0.466 # 44 설득력 -> 5
pearson: 0.459 	 qwk: 0.431 # 49 참신성  -> 6



---클리핑 추가해서 풍부한 근거만 다시 학습하기--

--근거의 풍부함--

pearson: 0.61 	 qwk: 0.5
7번째 모델, Epoch:10, pearson:0.610000, qwk:0.500000
pearson: 0.593 	 qwk: 0.479
pearson: 0.576 	 qwk: 0.463
pearson: 0.555 	 qwk: 0.488
pearson: 0.556 	 qwk: 0.508
8번째 모델, Epoch:50, pearson:0.556000, qwk:0.508000
pearson: 0.432 	 qwk: 0.333
pearson: 0.481 	 qwk: 0.448
pearson: 0.439 	 qwk: 0.333

--근거의 풍부함--
--기울기 클리핑 삭제, Adam -> AdamW로 변경-- 

Epoch:2, pearson:0.000, qwk:0.000
9번째 모델, Epoch:2, pearson:0.606000, qwk:0.468000
Epoch:4, pearson:0.606, qwk:0.468
Epoch:6, pearson:0.606, qwk:0.468
Epoch:8, pearson:0.606, qwk:0.468
10번째 모델, Epoch:8, pearson:0.601000, qwk:0.489000
Epoch:10, pearson:0.601, qwk:0.489
Epoch:12, pearson:0.601, qwk:0.489
Epoch:14, pearson:0.601, qwk:0.489
Epoch:16, pearson:0.601, qwk:0.489
Epoch:18, pearson:0.601, qwk:0.489
Epoch:20, pearson:0.601, qwk:0.489
11번째 모델, Epoch:20, pearson:0.607000, qwk:0.483000
Epoch:22, pearson:0.607, qwk:0.483
Epoch:24, pearson:0.607, qwk:0.483
Epoch:26, pearson:0.607, qwk:0.483
Epoch:28, pearson:0.607, qwk:0.483
Epoch:30, pearson:0.607, qwk:0.483
Epoch:32, pearson:0.607, qwk:0.483
Epoch:34, pearson:0.607, qwk:0.483
Epoch:36, pearson:0.607, qwk:0.483
Epoch:38, pearson:0.607, qwk:0.483
Epoch:40, pearson:0.607, qwk:0.483
Epoch:42, pearson:0.607, qwk:0.483
Epoch:44, pearson:0.607, qwk:0.483
Epoch:46, pearson:0.607, qwk:0.483
Epoch:48, pearson:0.607, qwk:0.483
Epoch:50, pearson:0.607, qwk:0.483
Epoch:52, pearson:0.607, qwk:0.483
Epoch:54, pearson:0.607, qwk:0.483
Epoch:56, pearson:0.607, qwk:0.483
Epoch:58, pearson:0.607, qwk:0.483
Epoch:60, pearson:0.607, qwk:0.483
Epoch:62, pearson:0.607, qwk:0.483
Epoch:64, pearson:0.607, qwk:0.483
Epoch:66, pearson:0.607, qwk:0.483
Epoch:68, pearson:0.607, qwk:0.483

--근거의 풍부함--
fit2: Adam + lr 스케줄러 추가, 기울기 클리핑 삭제

Epoch:2, pearson:0.531, qwk:0.417
12번째 모델, Epoch:2, pearson:0.531000, qwk:0.417000
Epoch:4, pearson:0.266, qwk:0.035
Epoch:6, pearson:0.413, qwk:0.089
Epoch:8, pearson:0.492, qwk:0.342
Epoch:10, pearson:0.516, qwk:0.371
Epoch:12, pearson:0.517, qwk:0.383
Epoch:14, pearson:0.495, qwk:0.408
Epoch:16, pearson:0.565, qwk:0.516
13번째 모델, Epoch:16, pearson:0.565000, qwk:0.516000
Epoch:18, pearson:0.503, qwk:0.335
Epoch:20, pearson:0.514, qwk:0.317
Epoch:22, pearson:0.518, qwk:0.369
Epoch:24, pearson:0.532, qwk:0.464
Epoch:26, pearson:0.530, qwk:0.507
Epoch:28, pearson:0.529, qwk:0.471
Epoch:30, pearson:0.493, qwk:0.430
Epoch:32, pearson:0.512, qwk:0.483
Epoch:34, pearson:0.521, qwk:0.480
Epoch:36, pearson:0.504, qwk:0.465
Epoch:38, pearson:0.537, qwk:0.511
Epoch:40, pearson:0.495, qwk:0.480
Epoch:42, pearson:0.537, qwk:0.509
Epoch:44, pearson:0.535, qwk:0.497
Epoch:46, pearson:0.535, qwk:0.476
Epoch:48, pearson:0.551, qwk:0.486
Epoch:50, pearson:0.540, qwk:0.462
Epoch:52, pearson:0.524, qwk:0.434
Epoch:54, pearson:0.551, qwk:0.505
Epoch:56, pearson:0.538, qwk:0.447
Epoch:58, pearson:0.543, qwk:0.469
Epoch:60, pearson:0.554, qwk:0.509
Epoch:62, pearson:0.533, qwk:0.454
Epoch:64, pearson:0.549, qwk:0.499
Epoch:66, pearson:0.561, qwk:0.520
14번째 모델, Epoch:66, pearson:0.561000, qwk:0.520000
Epoch:68, pearson:0.569, qwk:0.522
15번째 모델, Epoch:68, pearson:0.569000, qwk:0.522000
Epoch:70, pearson:0.556, qwk:0.502
Epoch:72, pearson:0.560, qwk:0.505
Epoch:74, pearson:0.571, qwk:0.536
16번째 모델, Epoch:74, pearson:0.571000, qwk:0.536000
Epoch:76, pearson:0.555, qwk:0.536
Epoch:78, pearson:0.559, qwk:0.506
Epoch:80, pearson:0.577, qwk:0.550
17번째 모델, Epoch:80, pearson:0.577000, qwk:0.550000

--근거의 풍부함--
--옵티마이져 RAdam 사용--

Epoch:2, pearson:0.694, qwk:0.378
18번째 모델, Epoch:2, pearson:0.694000, qwk:0.378000
Epoch:4, pearson:0.680, qwk:0.385
19번째 모델, Epoch:4, pearson:0.680000, qwk:0.385000