pearson: 0.687 qwk: 0.641

pearson: 0.461 	 qwk: 0.262
pearson: 0.434 	 qwk: 0.21
pearson: 0.421 	 qwk: 0.346


1,2,3
pearson: 0.504 	 qwk: 0.465
pearson: 0.492 	 qwk: 0.478
pearson: 0.587 	 qwk: 0.468
논리성 예측 점수 : 43.02점
참신성 예측 점수 : 9.46점
설득력 예측 점수 : 54.29점



--풍부한 근거--
13 ~ 21 (한번 맞는지 확인해볼 필요가 있다.)

pearson: 0.48 	 qwk: 0.43 13 x
pearson: nan 	 qwk: 0.0 14 13
pearson: 0.497 	 qwk: 0.34 15 14
pearson: 0.543 	 qwk: 0.422 16 15
pearson: 0.507 	 qwk: 0.451 17 16
pearson: 0.536 	 qwk: 0.52 18 17
pearson: 0.585 	 qwk: 0.583 19 18      # best
pearson: 0.53 	 qwk: 0.493 20 19
pearson: 0.565 	 qwk: 0.461 21 20

22

--논리성--

pearson: 0.516 	 qwk: 0.394 
pearson: 0.526 	 qwk: 0.427 
pearson: 0.529 	 qwk: 0.431 24    # best
pearson: 0.459 	 qwk: 0.363 
pearson: 0.383 	 qwk: 0.257
pearson: 0.413 	 qwk: 0.321
pearson: 0.321 	 qwk: 0.221
pearson: 0.428 	 qwk: 0.386

--참신성--

pearson: 0.477 	 qwk: 0.3
pearson: 0.376 	 qwk: 0.343
pearson: 0.436 	 qwk: 0.405
pearson: 0.422 	 qwk: 0.417
pearson: 0.414 	 qwk: 0.39
pearson: 0.424 	 qwk: 0.413 36
pearson: 0.389 	 qwk: 0.385
pearson: 0.445 	 qwk: 0.421 38     # best

--설득력--

pearson: 0.53 	 qwk: 0.355
pearson: 0.485 	 qwk: 0.328 41
pearson: 0.483 	 qwk: 0.337
pearson: 0.462 	 qwk: 0.375
pearson: 0.516 	 qwk: 0.466  44     # best
pearson: 0.485 	 qwk: 0.471
pearson: 0.502 	 qwk: 0.447
pearson: 0.454 	 qwk: 0.417




----------------
참신성 30에폭 추가 학습
49~

--best--

pearson: 0.529 	 qwk: 0.431 # 24 논리성 -> 4
pearson: 0.585 	 qwk: 0.583 # 18 풍부한 근거 -> 실수로 삭제 다시 학습 필요
pearson: 0.516 	 qwk: 0.466 # 44 설득력 -> 5
pearson: 0.459 	 qwk: 0.431 # 49 참신성  -> 6



---클리핑 추가해서 풍부한 근거만 다시 학습하기--

--근거의 풍부함--

pearson: 0.61 	 qwk: 0.5
7번째 모델, Epoch:10, pearson:0.610000, qwk:0.500000
pearson: 0.593 	 qwk: 0.479
pearson: 0.576 	 qwk: 0.463
pearson: 0.555 	 qwk: 0.488
pearson: 0.556 	 qwk: 0.508
8번째 모델, Epoch:50, pearson:0.556000, qwk:0.508000
pearson: 0.432 	 qwk: 0.333
pearson: 0.481 	 qwk: 0.448
pearson: 0.439 	 qwk: 0.333

--근거의 풍부함--
--기울기 클리핑 삭제, Adam -> AdamW로 변경-- 

Epoch:2, pearson:0.000, qwk:0.000
9번째 모델, Epoch:2, pearson:0.606000, qwk:0.468000
Epoch:4, pearson:0.606, qwk:0.468
Epoch:6, pearson:0.606, qwk:0.468
Epoch:8, pearson:0.606, qwk:0.468
10번째 모델, Epoch:8, pearson:0.601000, qwk:0.489000
Epoch:10, pearson:0.601, qwk:0.489
Epoch:12, pearson:0.601, qwk:0.489
Epoch:14, pearson:0.601, qwk:0.489
Epoch:16, pearson:0.601, qwk:0.489
Epoch:18, pearson:0.601, qwk:0.489
Epoch:20, pearson:0.601, qwk:0.489
11번째 모델, Epoch:20, pearson:0.607000, qwk:0.483000
Epoch:22, pearson:0.607, qwk:0.483
Epoch:24, pearson:0.607, qwk:0.483
Epoch:26, pearson:0.607, qwk:0.483
Epoch:28, pearson:0.607, qwk:0.483
Epoch:30, pearson:0.607, qwk:0.483
Epoch:32, pearson:0.607, qwk:0.483
Epoch:34, pearson:0.607, qwk:0.483
Epoch:36, pearson:0.607, qwk:0.483
Epoch:38, pearson:0.607, qwk:0.483
Epoch:40, pearson:0.607, qwk:0.483
Epoch:42, pearson:0.607, qwk:0.483
Epoch:44, pearson:0.607, qwk:0.483
Epoch:46, pearson:0.607, qwk:0.483
Epoch:48, pearson:0.607, qwk:0.483
Epoch:50, pearson:0.607, qwk:0.483
Epoch:52, pearson:0.607, qwk:0.483
Epoch:54, pearson:0.607, qwk:0.483
Epoch:56, pearson:0.607, qwk:0.483
Epoch:58, pearson:0.607, qwk:0.483
Epoch:60, pearson:0.607, qwk:0.483
Epoch:62, pearson:0.607, qwk:0.483
Epoch:64, pearson:0.607, qwk:0.483
Epoch:66, pearson:0.607, qwk:0.483
Epoch:68, pearson:0.607, qwk:0.483

--근거의 풍부함--
fit2: Adam + lr 스케줄러 추가, 기울기 클리핑 삭제

Epoch:2, pearson:0.531, qwk:0.417
12번째 모델, Epoch:2, pearson:0.531000, qwk:0.417000
Epoch:4, pearson:0.266, qwk:0.035
Epoch:6, pearson:0.413, qwk:0.089
Epoch:8, pearson:0.492, qwk:0.342
Epoch:10, pearson:0.516, qwk:0.371
Epoch:12, pearson:0.517, qwk:0.383
Epoch:14, pearson:0.495, qwk:0.408
Epoch:16, pearson:0.565, qwk:0.516
13번째 모델, Epoch:16, pearson:0.565000, qwk:0.516000
Epoch:18, pearson:0.503, qwk:0.335
Epoch:20, pearson:0.514, qwk:0.317
Epoch:22, pearson:0.518, qwk:0.369
Epoch:24, pearson:0.532, qwk:0.464
Epoch:26, pearson:0.530, qwk:0.507
Epoch:28, pearson:0.529, qwk:0.471
Epoch:30, pearson:0.493, qwk:0.430
Epoch:32, pearson:0.512, qwk:0.483
Epoch:34, pearson:0.521, qwk:0.480
Epoch:36, pearson:0.504, qwk:0.465
Epoch:38, pearson:0.537, qwk:0.511
Epoch:40, pearson:0.495, qwk:0.480
Epoch:42, pearson:0.537, qwk:0.509
Epoch:44, pearson:0.535, qwk:0.497
Epoch:46, pearson:0.535, qwk:0.476
Epoch:48, pearson:0.551, qwk:0.486
Epoch:50, pearson:0.540, qwk:0.462
Epoch:52, pearson:0.524, qwk:0.434
Epoch:54, pearson:0.551, qwk:0.505
Epoch:56, pearson:0.538, qwk:0.447
Epoch:58, pearson:0.543, qwk:0.469
Epoch:60, pearson:0.554, qwk:0.509
Epoch:62, pearson:0.533, qwk:0.454
Epoch:64, pearson:0.549, qwk:0.499
Epoch:66, pearson:0.561, qwk:0.520
14번째 모델, Epoch:66, pearson:0.561000, qwk:0.520000
Epoch:68, pearson:0.569, qwk:0.522
15번째 모델, Epoch:68, pearson:0.569000, qwk:0.522000
Epoch:70, pearson:0.556, qwk:0.502
Epoch:72, pearson:0.560, qwk:0.505
Epoch:74, pearson:0.571, qwk:0.536
16번째 모델, Epoch:74, pearson:0.571000, qwk:0.536000
Epoch:76, pearson:0.555, qwk:0.536
Epoch:78, pearson:0.559, qwk:0.506
Epoch:80, pearson:0.577, qwk:0.550
17번째 모델, Epoch:80, pearson:0.577000, qwk:0.550000

--근거의 풍부함--
--옵티마이져 RAdam 사용--

Epoch:2, pearson:0.694, qwk:0.378
18번째 모델, Epoch:2, pearson:0.694000, qwk:0.378000
Epoch:4, pearson:0.680, qwk:0.385
19번째 모델, Epoch:4, pearson:0.680000, qwk:0.385000
Epoch:6, pearson:0.711, qwk:0.331
20번째 모델, Epoch:6, pearson:0.711000, qwk:0.331000
Epoch:8, pearson:0.642, qwk:0.231
Epoch:10, pearson:0.674, qwk:0.446
21번째 모델, Epoch:10, pearson:0.674000, qwk:0.446000
Epoch:12, pearson:0.655, qwk:0.301
Epoch:14, pearson:0.652, qwk:0.343
Epoch:16, pearson:0.644, qwk:0.380
Epoch:18, pearson:0.608, qwk:0.385
Epoch:20, pearson:0.654, qwk:0.435
Epoch:22, pearson:0.622, qwk:0.311
Epoch:24, pearson:0.586, qwk:0.460
22번째 모델, Epoch:24, pearson:0.586000, qwk:0.460000
Epoch:26, pearson:0.590, qwk:0.468
23번째 모델, Epoch:26, pearson:0.590000, qwk:0.468000
Epoch:28, pearson:0.587, qwk:0.373
Epoch:30, pearson:0.598, qwk:0.390
24번째 모델, Epoch:30, pearson:0.598000, qwk:0.390000
Epoch:32, pearson:0.579, qwk:0.382
Epoch:34, pearson:0.614, qwk:0.388
25번째 모델, Epoch:34, pearson:0.614000, qwk:0.388000
Epoch:36, pearson:0.626, qwk:0.447
26번째 모델, Epoch:36, pearson:0.626000, qwk:0.447000
Epoch:38, pearson:0.628, qwk:0.490
27번째 모델, Epoch:38, pearson:0.628000, qwk:0.490000
Epoch:40, pearson:0.621, qwk:0.528
28번째 모델, Epoch:40, pearson:0.621000, qwk:0.528000
Epoch:42, pearson:0.644, qwk:0.547
29번째 모델, Epoch:42, pearson:0.644000, qwk:0.547000
Epoch:44, pearson:0.629, qwk:0.510
Epoch:46, pearson:0.629, qwk:0.530
Epoch:48, pearson:0.641, qwk:0.502
Epoch:50, pearson:0.624, qwk:0.525
Epoch:52, pearson:0.636, qwk:0.511
Epoch:54, pearson:0.628, qwk:0.517
Epoch:56, pearson:0.627, qwk:0.516
Epoch:58, pearson:0.628, qwk:0.517
Epoch:60, pearson:0.627, qwk:0.517
Epoch:62, pearson:0.644, qwk:0.491
Epoch:64, pearson:0.617, qwk:0.469
Epoch:66, pearson:0.618, qwk:0.470
Epoch:68, pearson:0.636, qwk:0.512
Epoch:70, pearson:0.628, qwk:0.501
Epoch:72, pearson:0.638, qwk:0.533
Epoch:74, pearson:0.635, qwk:0.528
Epoch:76, pearson:0.641, qwk:0.521
Epoch:78, pearson:0.640, qwk:0.540
Epoch:80, pearson:0.636, qwk:0.522

--근거의 풍부함--
Adam + lr 스케줄러 추가, 기울기 클리핑 삭제

Epoch:2, pearson:0.654, qwk:0.464
30번째 모델, Epoch:2, pearson:0.654000, qwk:0.464000
Epoch:4, pearson:0.667, qwk:0.445
31번째 모델, Epoch:4, pearson:0.667000, qwk:0.445000
Epoch:6, pearson:0.670, qwk:0.464
32번째 모델, Epoch:6, pearson:0.670000, qwk:0.464000
Epoch:8, pearson:0.667, qwk:0.456
Epoch:10, pearson:0.667, qwk:0.468
33번째 모델, Epoch:10, pearson:0.667000, qwk:0.468000
Epoch:12, pearson:0.505, qwk:0.129
Epoch:14, pearson:0.666, qwk:0.178
Epoch:16, pearson:0.666, qwk:0.183
Epoch:18, pearson:0.669, qwk:0.178
34번째 모델, Epoch:18, pearson:0.669000, qwk:0.178000

--근거의 풍부함--
바로 위랑 같은데 코사인 유사도만 torch 이용해서 똑같이 구현함. 혹시 성능차이 있나 실험하는 것
Epoch:2, pearson:0.691, qwk:0.422
35번째 모델, Epoch:2, pearson:0.691000, qwk:0.422000
Epoch:4, pearson:0.633, qwk:0.144
Epoch:6, pearson:0.597, qwk:0.127
Epoch:8, pearson:0.642, qwk:0.461
36번째 모델, Epoch:8, pearson:0.642000, qwk:0.461000
Epoch:10, pearson:0.618, qwk:0.474
37번째 모델, Epoch:10, pearson:0.618000, qwk:0.474000
Epoch:12, pearson:0.598, qwk:0.455
Epoch:14, pearson:0.592, qwk:0.502
38번째 모델, Epoch:14, pearson:0.592000, qwk:0.502000
Epoch:16, pearson:0.560, qwk:0.342



--근거의 풍부함--
--위랑 같음--
Epoch:2, pearson:0.673, qwk:0.464
39번째 모델, Epoch:2, pearson:0.673000, qwk:0.464000
Epoch:4, pearson:0.672, qwk:0.450
Epoch:6, pearson:0.670, qwk:0.485
40번째 모델, Epoch:6, pearson:0.670000, qwk:0.485000
Epoch:8, pearson:0.679, qwk:0.415
41번째 모델, Epoch:8, pearson:0.679000, qwk:0.415000

--근거의 풍부함--
--RAdam 사용--
Epoch:2, pearson:0.697, qwk:0.372
42번째 모델, Epoch:2, pearson:0.697000, qwk:0.372000
Epoch:4, pearson:0.683, qwk:0.428
43번째 모델, Epoch:4, pearson:0.683000, qwk:0.428000
Epoch:6, pearson:0.660, qwk:0.187
Epoch:8, pearson:0.669, qwk:0.179
Epoch:10, pearson:0.601, qwk:0.132
Epoch:12, pearson:0.524, qwk:0.092
Epoch:14, pearson:0.650, qwk:0.194
Epoch:16, pearson:0.608, qwk:0.460
44번째 모델, Epoch:16, pearson:0.608000, qwk:0.460000